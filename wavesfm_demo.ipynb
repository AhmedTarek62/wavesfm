{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "404ed1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import random\n",
    "import importlib.util\n",
    "import os\n",
    "\n",
    "# --- clone repo if needed (Colab-friendly) ---\n",
    "REPO_URL = \"https://github.com/AhmedTarek62/wavesfm\"\n",
    "if (Path.cwd() / \"main_finetune.py\").exists():\n",
    "    REPO_DIR = Path.cwd()\n",
    "else:\n",
    "    REPO_DIR = Path(\"/content/wavesfm\")\n",
    "    if not REPO_DIR.exists():\n",
    "        subprocess.run([\"git\", \"clone\", REPO_URL, str(REPO_DIR)], check=True)\n",
    "    os.chdir(REPO_DIR)\n",
    "\n",
    "if str(REPO_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_DIR))\n",
    "\n",
    "# --- install missing deps (Colab) ---\n",
    "def _ensure_pkg(module, pip_name=None):\n",
    "    if importlib.util.find_spec(module) is None:\n",
    "        pkg = pip_name or module\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=True)\n",
    "\n",
    "if importlib.util.find_spec(\"torch\") is None:\n",
    "    subprocess.run([\n",
    "        sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "        \"torch\", \"torchvision\", \"torchaudio\"\n",
    "    ], check=True)\n",
    "\n",
    "for mod, pip_name in [\n",
    "    (\"numpy\", \"numpy\"),\n",
    "    (\"matplotlib\", \"matplotlib\"),\n",
    "    (\"h5py\", \"h5py\"),\n",
    "    (\"scipy\", \"scipy\"),\n",
    "    (\"tqdm\", \"tqdm\"),\n",
    "    (\"timm\", \"timm\"),\n",
    "    (\"pandas\", \"pandas\"),\n",
    "    (\"PIL\", \"pillow\"),\n",
    "    (\"gdown\", \"gdown\"),\n",
    "]:\n",
    "    _ensure_pkg(mod, pip_name)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd68d85",
   "metadata": {},
   "source": [
    "## Download raw datasets\n",
    "\n",
    "This notebook supports a **subset of tasks** only. For deeper experiments (more tasks, configs, and training options), refer to the full repository: https://github.com/AhmedTarek62/wavesfm\n",
    "\n",
    "### Vision tasks\n",
    "- `sensing` — EfficientFi (Human Activity Sensing): https://github.com/xyanchen/WiFi-CSI-Sensing-Benchmark  \n",
    "- `deepmimo-los` — DeepMIMO LoS/NLoS classification (generated locally via `preprocess_deepmimo.py`; no manual downloads)\n",
    "\n",
    "### IQ tasks\n",
    "- `rml` — RML 2022 (Modulation Classification): https://github.com/venkateshsathya/RML22  \n",
    "- `uwb-industrial` — UWB Industrial Positioning: https://owncloud.fraunhofer.de/index.php/s/AXFjGY9IhswfBSa/download\n",
    "\n",
    "#\n",
    "> **Note:** Links were valid at the time this notebook was published. If a link breaks, please use the corresponding project page above to locate the latest download instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e265cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "TASK = \"rml\"  # {\"sensing\", \"deepmimo-los\", \"rml\", \"uwb-industrial\"}\n",
    "DOWNLOAD_DATA = True      # set False if you already downloaded the raw files\n",
    "SEED = 2\n",
    "VAL_SPLIT = 0.2\n",
    "STRATIFIED_TASKS = {\"deepmimo-los\"}\n",
    "SMOOTH_TASKS = {\"sensing\": 0.1}\n",
    "\n",
    "# Paths\n",
    "DATA_ROOT = Path(\"data\")\n",
    "RAW_ROOT = DATA_ROOT / \"raw\"\n",
    "CACHE_ROOT = DATA_ROOT / \"cache\"\n",
    "OUTPUT_DIR = Path(\"runs/demo\") / TASK\n",
    "CHECKPOINT_DIR = Path(\"checkpoints\")\n",
    "\n",
    "for p in (RAW_ROOT, CACHE_ROOT, CHECKPOINT_DIR, OUTPUT_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Raw dataset locations (after download/extraction)\n",
    "HAS_DIR = RAW_ROOT / \"NTU-Fi_HAR\"              # EfficientFi HAS (sensing)\n",
    "RML_DATA_FILE = RAW_ROOT / \"rml2022\"            # RML 2022 (rml)\n",
    "UWB_INDUSTRIAL_DATA_FILE = RAW_ROOT / \"industrial_training.pkl\"  # UWB Industrial (uwb-industrial)\n",
    "DEEPMIMO_SCENARIOS_DIR = RAW_ROOT / \"deepmimo_scenarios\"          # DeepMIMO scenarios clone target\n",
    "DEEPMIMO_IMG_SIZE = 32\n",
    "DEEPMIMO_CLONE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa912dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional dataset download\n",
    "\n",
    "def run(cmd):\n",
    "    # Run a shell command and fail loudly if it errors.\n",
    "    subprocess.run(cmd, check=True)\n",
    "\n",
    "if not DOWNLOAD_DATA:\n",
    "    print(\"DOWNLOAD_DATA=False → assuming raw files already exist under:\", RAW_ROOT)\n",
    "\n",
    "elif TASK == \"sensing\":\n",
    "    # EfficientFi HAS (Google Drive zip)\n",
    "    HAS_URL = \"https://drive.google.com/file/d/1DszE7byFzlpyI9gZvmVn51fTr8L1iZaI/view?usp=drive_link\"\n",
    "    zip_path = RAW_ROOT / \"has.zip\"\n",
    "    run([\"gdown\", \"--fuzzy\", \"-O\", str(zip_path), HAS_URL])\n",
    "    run([\"unzip\", \"-o\", str(zip_path), \"-d\", str(RAW_ROOT)])\n",
    "    run([\"rm\", str(zip_path)])\n",
    "\n",
    "elif TASK == \"deepmimo-los\":\n",
    "    print(\"DeepMIMO is generated during preprocessing; no download step needed.\")\n",
    "\n",
    "elif TASK == \"rml\":\n",
    "    # RML 2022 (Google Drive)\n",
    "    RML_URL = \"https://drive.google.com/file/d/1wrqnanHbmdFiP3DqaBjSVcBxoQ0nzD-a/view?usp=drive_link\"\n",
    "    out_path = RAW_ROOT / \"rml2022\"  # keep as downloaded filename; downstream code can point to it\n",
    "    run([\"gdown\", \"--fuzzy\", \"-O\", str(out_path), RML_URL])\n",
    "\n",
    "elif TASK == \"uwb-industrial\":\n",
    "    # UWB Industrial Positioning (Fraunhofer ownCloud)\n",
    "    UWB_URL = \"https://owncloud.fraunhofer.de/index.php/s/AXFjGY9IhswfBSa/download\"\n",
    "    pkl_path = UWB_INDUSTRIAL_DATA_FILE\n",
    "    run([\"wget\", \"-O\", str(pkl_path), UWB_URL])\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"Unknown TASK={TASK!r}. Expected one of: sensing, deepmimo-los, rml, uwb-industrial.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40606f7",
   "metadata": {},
   "source": [
    "## Create preprocessed .h5 cache from raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d20f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- cache path ---\n",
    "if TASK == \"sensing\":\n",
    "    CACHE_PATH = CACHE_ROOT / \"has.h5\"\n",
    "elif TASK == \"deepmimo-los\":\n",
    "    CACHE_PATH = CACHE_ROOT / \"deepmimo.h5\"\n",
    "elif TASK == \"rml\":\n",
    "    CACHE_PATH = CACHE_ROOT / \"rml2022.h5\"\n",
    "elif TASK == \"uwb-industrial\":\n",
    "    CACHE_PATH = CACHE_ROOT / \"uwb-industrial.h5\"\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported TASK: {TASK}\")\n",
    "\n",
    "print(\"Task:\", TASK)\n",
    "print(\"Cache path:\", CACHE_PATH)\n",
    "\n",
    "# --- preprocess (skip this cell if you already have CACHE_PATH) ---\n",
    "if TASK == \"sensing\":\n",
    "    subprocess.run([\n",
    "        sys.executable, \"preprocessing/preprocess_csi_sensing.py\",\n",
    "        \"--data-path\", str(HAS_DIR),\n",
    "        \"--output\", str(CACHE_PATH),\n",
    "        \"--overwrite\"], check=True)\n",
    "\n",
    "elif TASK == \"deepmimo-los\":\n",
    "    deepmimo_cmd = [\n",
    "        sys.executable, \"preprocessing/preprocess_deepmimo.py\",\n",
    "        \"--output\", str(CACHE_PATH),\n",
    "        \"--dataset-folder\", str(DEEPMIMO_SCENARIOS_DIR),\n",
    "        \"--resize-size\", str(DEEPMIMO_IMG_SIZE),\n",
    "        \"--overwrite\",\n",
    "    ]\n",
    "    if DEEPMIMO_CLONE:\n",
    "        deepmimo_cmd.append(\"--clone-scenarios\")\n",
    "    subprocess.run(deepmimo_cmd, check=True)\n",
    "\n",
    "elif TASK == \"rml\":\n",
    "    if not RML_DATA_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Missing RML file at {RML_DATA_FILE}\")\n",
    "    subprocess.run([\n",
    "        sys.executable, \"preprocessing/preprocess_rml.py\",\n",
    "        \"--data-file\", str(RML_DATA_FILE),\n",
    "        \"--version\", \"2022\",\n",
    "        \"--output\", str(CACHE_PATH),\n",
    "        \"--overwrite\",\n",
    "    ], check=True)\n",
    "\n",
    "elif TASK == \"uwb-industrial\":\n",
    "    if not UWB_INDUSTRIAL_DATA_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Missing UWB-Industrial file at {UWB_INDUSTRIAL_DATA_FILE}\")\n",
    "    subprocess.run([\n",
    "        sys.executable, \"preprocessing/preprocess_ipin_loc.py\",\n",
    "        \"--data-path\", str(UWB_INDUSTRIAL_DATA_FILE),\n",
    "        \"--output\", str(CACHE_PATH),\n",
    "        \"--overwrite\",\n",
    "    ], check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3df4bd",
   "metadata": {},
   "source": [
    "## Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ec85b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from types import SimpleNamespace\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from data import build_datasets\n",
    "from main_finetune import build_model\n",
    "\n",
    "\n",
    "def _unwrap_subset_with_indices(ds):\n",
    "    if not isinstance(ds, Subset):\n",
    "        return ds, None\n",
    "    indices = list(ds.indices)\n",
    "    base = ds.dataset\n",
    "    while isinstance(base, Subset):\n",
    "        indices = [base.indices[i] for i in indices]\n",
    "        base = base.dataset\n",
    "    return base, np.asarray(indices, dtype=np.int64)\n",
    "\n",
    "\n",
    "def _load_label_names(ds, task_info):\n",
    "    base, _ = _unwrap_subset_with_indices(ds)\n",
    "    labels = getattr(base, \"labels\", None)\n",
    "    if labels:\n",
    "        return list(labels)\n",
    "    h5_path = getattr(base, \"h5_path\", None)\n",
    "    if h5_path:\n",
    "        with h5py.File(h5_path, \"r\") as h5:\n",
    "            raw = h5.attrs.get(\"labels\", None)\n",
    "            if raw:\n",
    "                return list(json.loads(raw))\n",
    "            raw = h5.attrs.get(\"labels_los\", None)\n",
    "            if raw:\n",
    "                return list(json.loads(raw))\n",
    "    return [str(i) for i in range(task_info.num_outputs)]\n",
    "\n",
    "\n",
    "def plot_samples(ds, task_info, *, seed=0, num_show=6):\n",
    "    rng = random.Random(seed)\n",
    "    num_show = min(num_show, len(ds))\n",
    "    indices = rng.sample(range(len(ds)), num_show)\n",
    "\n",
    "    def _to_numpy(x):\n",
    "        if torch.is_tensor(x):\n",
    "            return x.detach().cpu().numpy()\n",
    "        return np.asarray(x)\n",
    "\n",
    "    def _label_text(label):\n",
    "        if torch.is_tensor(label):\n",
    "            label = label.detach().cpu().numpy()\n",
    "        label = np.asarray(label)\n",
    "        if label.shape == ():\n",
    "            return f\"label={int(label)}\"\n",
    "        return f\"label={np.array2string(label, precision=2, separator=',')}\"\n",
    "\n",
    "    def _plot_iq(ax, x):\n",
    "        # x expected as (2, L) or (2, A, L) or similar\n",
    "        if x.ndim >= 3:\n",
    "            i = np.asarray(x[0]).reshape(x.shape[1], -1)[0]\n",
    "            q = np.asarray(x[1]).reshape(x.shape[1], -1)[0]\n",
    "        elif x.ndim == 2:\n",
    "            i = x[0]\n",
    "            q = x[1]\n",
    "        else:\n",
    "            flat = x.reshape(-1)\n",
    "            half = flat.size // 2\n",
    "            i = flat[:half]\n",
    "            q = flat[half:]\n",
    "        ax.plot(i, label=\"I\")\n",
    "        ax.plot(q, label=\"Q\")\n",
    "        ax.legend(fontsize=6)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    def _plot_sample(ax, sample, label, modality):\n",
    "        x = _to_numpy(sample)\n",
    "        if modality in (\"iq\", \"cir\"):\n",
    "            _plot_iq(ax, x)\n",
    "        else:\n",
    "            if x.ndim == 3 and x.shape[0] in (1, 2, 3):\n",
    "                img = np.moveaxis(x, 0, -1)\n",
    "                if img.shape[-1] == 1:\n",
    "                    ax.imshow(img[..., 0], cmap=\"viridis\")\n",
    "                elif img.shape[-1] == 2:\n",
    "                    mag = np.sqrt(img[..., 0] ** 2 + img[..., 1] ** 2)\n",
    "                    ax.imshow(mag, cmap=\"viridis\")\n",
    "                else:\n",
    "                    ax.imshow(img)\n",
    "                ax.axis(\"off\")\n",
    "            elif x.ndim == 2:\n",
    "                ax.imshow(x, aspect=\"auto\", cmap=\"viridis\")\n",
    "                ax.axis(\"off\")\n",
    "            else:\n",
    "                ax.plot(x.flatten())\n",
    "                ax.axis(\"off\")\n",
    "        ax.set_title(_label_text(label))\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_show, figsize=(3 * num_show, 5))\n",
    "    if num_show == 1:\n",
    "        axes = [axes]\n",
    "    for ax, idx in zip(axes, indices):\n",
    "        sample, label = ds[idx]\n",
    "        _plot_sample(ax, sample, label, task_info.modality)\n",
    "\n",
    "    fig.suptitle(f\"{TASK} samples (train split)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pick_eval_ckpt(output_dir: Path) -> Path:\n",
    "    best = output_dir / \"best.pth\"\n",
    "    if best.exists():\n",
    "        return best\n",
    "    candidates = list(output_dir.glob(\"checkpoint_*.pth\"))\n",
    "    if candidates:\n",
    "        def _ckpt_epoch(path: Path) -> int:\n",
    "            match = re.search(r\"checkpoint_(\\d+)\\.pth\", path.name)\n",
    "            return int(match.group(1)) if match else -1\n",
    "        return max(candidates, key=_ckpt_epoch)\n",
    "    raise FileNotFoundError(f\"No checkpoints found in {output_dir}\")\n",
    "\n",
    "\n",
    "def load_eval_model(ckpt_path: Path, task_info, *, device: str = \"cpu\"):\n",
    "    model_args = SimpleNamespace(\n",
    "        model=\"vit_multi_small\",\n",
    "        global_pool=\"token\",\n",
    "        vis_patch=16,\n",
    "        vis_img_size=DEEPMIMO_IMG_SIZE if TASK.startswith(\"deepmimo\") else 224,\n",
    "        iq_segment_len=16,\n",
    "        iq_downsample=None,\n",
    "        iq_target_len=256,\n",
    "        use_conditional_ln=True,\n",
    "    )\n",
    "    model = build_model(model_args, task_info)\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "    state = ckpt.get(\"model\", ckpt) if isinstance(ckpt, dict) else ckpt\n",
    "    model.load_state_dict(state, strict=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(model, val_ds, task_info, *, batch_size=256, device=\"cpu\", annotate=True):\n",
    "    loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    num_classes = task_info.num_outputs\n",
    "    conf = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            samples, targets = batch[0].to(device), batch[1].to(device).long()\n",
    "            outputs = model(samples)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            for t, p in zip(targets.cpu().numpy(), preds.cpu().numpy()):\n",
    "                conf[int(t), int(p)] += 1\n",
    "\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        row_sums = conf.sum(axis=1, keepdims=True)\n",
    "        conf_norm = np.divide(conf, row_sums, out=np.zeros_like(conf, dtype=float), where=row_sums != 0)\n",
    "\n",
    "    label_names = _load_label_names(val_ds, task_info)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(conf_norm, cmap=\"Blues\", vmin=0.0, vmax=1.0)\n",
    "    ax.set_title(f\"Confusion Matrix ({TASK})\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks(range(num_classes))\n",
    "    ax.set_yticks(range(num_classes))\n",
    "    ax.set_xticklabels(label_names, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(label_names)\n",
    "\n",
    "    if annotate:\n",
    "        for i in range(num_classes):\n",
    "            for j in range(num_classes):\n",
    "                val = conf_norm[i, j]\n",
    "                ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_uwb_position_density(ds, *, max_points=5000, bins=60):\n",
    "    base, idxs = _unwrap_subset_with_indices(ds)\n",
    "    if not hasattr(base, \"loc_min\") or not hasattr(base, \"loc_max\"):\n",
    "        print(\"UWB location metadata missing; skipping density plot.\")\n",
    "        return\n",
    "\n",
    "    total = len(ds)\n",
    "    if total == 0:\n",
    "        print(\"No samples available for density plot.\")\n",
    "        return\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    if total > max_points:\n",
    "        sample_indices = rng.choice(total, size=max_points, replace=False)\n",
    "    else:\n",
    "        sample_indices = np.arange(total)\n",
    "\n",
    "    coords = []\n",
    "    for i in sample_indices:\n",
    "        _, loc = ds[int(i)]\n",
    "        if torch.is_tensor(loc):\n",
    "            loc = loc.detach().cpu().numpy()\n",
    "        loc = np.asarray(loc).reshape(-1)[:2]\n",
    "        coords.append(loc)\n",
    "\n",
    "    coords = np.asarray(coords, dtype=np.float32)\n",
    "    loc_min = np.asarray(base.loc_min[:2].cpu(), dtype=np.float32)\n",
    "    loc_max = np.asarray(base.loc_max[:2].cpu(), dtype=np.float32)\n",
    "    coords = (coords + 1.0) * 0.5 * (loc_max - loc_min) + loc_min\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    ax.hist2d(coords[:, 0], coords[:, 1], bins=bins, cmap=\"magma\")\n",
    "    ax.set_title(\"UWB-Industrial Position Density\")\n",
    "    ax.set_xlabel(\"X\")\n",
    "    ax.set_ylabel(\"Y\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_rml_accuracy_vs_snr(model, val_ds, *, batch_size=256, device=\"cpu\"):\n",
    "    base, idxs = _unwrap_subset_with_indices(val_ds)\n",
    "    snr_by_index = getattr(base, \"snr_by_index\", None)\n",
    "    if snr_by_index is None:\n",
    "        print(\"SNR metadata not found; skipping accuracy vs SNR plot.\")\n",
    "        return\n",
    "\n",
    "    snrs = np.asarray(snr_by_index, dtype=np.int16)\n",
    "    if idxs is not None:\n",
    "        snrs = snrs[idxs]\n",
    "\n",
    "    loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    offset = 0\n",
    "    correct = {}\n",
    "    total = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            samples, targets = batch[0].to(device), batch[1].to(device).long()\n",
    "            outputs = model(samples)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            batch_snrs = snrs[offset: offset + len(targets)]\n",
    "            offset += len(targets)\n",
    "\n",
    "            for snr_val, p, t in zip(batch_snrs, preds.cpu().numpy(), targets.cpu().numpy()):\n",
    "                snr_val = int(snr_val)\n",
    "                total[snr_val] = total.get(snr_val, 0) + 1\n",
    "                correct[snr_val] = correct.get(snr_val, 0) + int(p == t)\n",
    "\n",
    "    snr_levels = sorted(total.keys())\n",
    "    acc = [correct[s] / total[s] for s in snr_levels]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    ax.plot(snr_levels, acc, marker=\"o\")\n",
    "    ax.set_title(\"RML Accuracy vs SNR\")\n",
    "    ax.set_xlabel(\"SNR\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eedb90",
   "metadata": {},
   "source": [
    "## Visualize cached samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, task_info = build_datasets(\n",
    "    TASK,\n",
    "    str(CACHE_PATH),\n",
    "    val_path=None,\n",
    "    val_split=VAL_SPLIT,\n",
    "    stratified_split=TASK in STRATIFIED_TASKS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "plot_samples(train_ds, task_info, seed=SEED)\n",
    "print(\"Task info:\", task_info.modality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf31f2f",
   "metadata": {},
   "source": [
    "## Download pretrained checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf357d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hub import download_pretrained\n",
    "\n",
    "HF_REPO = \"ahmedaboulfo/wavesfm\"\n",
    "HF_FILE = \"wavesfm-v1p0.pth\"\n",
    "\n",
    "PRETRAINED_PATH = Path(\n",
    "    download_pretrained(repo_id=HF_REPO, filename=HF_FILE, cache_dir=str(CHECKPOINT_DIR))\n",
    ")\n",
    "print(\"Downloaded to:\", PRETRAINED_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235d715d",
   "metadata": {},
   "source": [
    "## Finetune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82099399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- finetune (match run_finetune_all.py defaults, seed 0) ---\n",
    "EPOCHS_BY_TASK = {\"rml\": 50,}\n",
    "DEFAULT_EPOCHS = 100\n",
    "BATCH_SIZE_BY_TASK = {\"rml\": 2048,}\n",
    "DEFAULT_BATCH = 256\n",
    "\n",
    "epochs = EPOCHS_BY_TASK.get(TASK, DEFAULT_EPOCHS)\n",
    "batch_size = BATCH_SIZE_BY_TASK.get(TASK, DEFAULT_BATCH)\n",
    "\n",
    "train_cmd = [\n",
    "    sys.executable, \"main_finetune.py\",\n",
    "    \"--task\", TASK,\n",
    "    \"--train-data\", str(CACHE_PATH),\n",
    "    \"--output-dir\", str(OUTPUT_DIR),\n",
    "    \"--batch-size\", str(batch_size),\n",
    "    \"--num-workers\", \"2\",\n",
    "    \"--epochs\", str(epochs),\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--val-split\", str(VAL_SPLIT),\n",
    "    \"--model\", \"vit_multi_small\",\n",
    "    \"--warmup-epochs\", \"5\",\n",
    "    \"--use-conditional-ln\",\n",
    "    \"--finetune\", str(PRETRAINED_PATH),\n",
    "]\n",
    "\n",
    "if TASK in STRATIFIED_TASKS:\n",
    "    train_cmd += [\"--stratified-split\", \"--class-weights\"]\n",
    "if TASK in SMOOTH_TASKS:\n",
    "    train_cmd += [\"--smoothing\", str(SMOOTH_TASKS[TASK])]\n",
    "if TASK.startswith(\"deepmimo\"):\n",
    "    train_cmd += [\"--vis-img-size\", str(DEEPMIMO_IMG_SIZE)]\n",
    "\n",
    "subprocess.run(train_cmd, check=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34674876",
   "metadata": {},
   "source": [
    "## Load evaluation checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca8e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt = pick_eval_ckpt(OUTPUT_DIR)\n",
    "print(f\"Eval checkpoint: {best_ckpt}\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_ds, val_ds, task_info = build_datasets(\n",
    "    TASK,\n",
    "    str(CACHE_PATH),\n",
    "    val_path=None,\n",
    "    val_split=VAL_SPLIT,\n",
    "    stratified_split=TASK in STRATIFIED_TASKS,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "eval_model = load_eval_model(best_ckpt, task_info, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c36b5",
   "metadata": {},
   "source": [
    "## Evaluation & plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e8944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- eval-only metrics ---\n",
    "eval_cmd = [\n",
    "    sys.executable, \"main_finetune.py\",\n",
    "    \"--task\", TASK,\n",
    "    \"--train-data\", str(CACHE_PATH),\n",
    "    \"--batch-size\", str(batch_size),\n",
    "    \"--num-workers\", \"2\",\n",
    "    \"--seed\", str(SEED),\n",
    "    \"--val-split\", str(VAL_SPLIT),\n",
    "    \"--model\", \"vit_multi_small\",\n",
    "    \"--warmup-epochs\", \"5\",\n",
    "    \"--use-conditional-ln\",\n",
    "    \"--eval-only\",\n",
    "    \"--finetune\", str(best_ckpt),\n",
    "]\n",
    "\n",
    "if TASK in STRATIFIED_TASKS:\n",
    "    eval_cmd += [\"--stratified-split\", \"--class-weights\"]\n",
    "if TASK in SMOOTH_TASKS:\n",
    "    eval_cmd += [\"--smoothing\", str(SMOOTH_TASKS[TASK])]\n",
    "if TASK.startswith(\"deepmimo\"):\n",
    "    eval_cmd += [\"--vis-img-size\", str(DEEPMIMO_IMG_SIZE)]\n",
    "\n",
    "subprocess.run(eval_cmd, check=True)\n",
    "\n",
    "# --- plots ---\n",
    "if task_info.target_type == \"classification\":\n",
    "    plot_confusion_matrix(eval_model, val_ds, task_info, batch_size=batch_size, device=device, annotate=(TASK != \"rml\"))\n",
    "\n",
    "if TASK == \"uwb-industrial\":\n",
    "    plot_uwb_position_density(val_ds)\n",
    "\n",
    "if TASK == \"rml\":\n",
    "    plot_rml_accuracy_vs_snr(eval_model, val_ds, batch_size=batch_size, device=device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
